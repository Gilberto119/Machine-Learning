# -*- coding: utf-8 -*-
"""RegresionLogistica.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Uk6xnMsUYfYDHkP1dFdzBgyzxdKmjxjP
"""

import pandas as pd
import numpy as np

import matplotlib as mpl
import matplotlib.pyplot as plt
import seaborn as sns
import missingno as msno
import plotly.express as px
import plotly.figure_factory as ff
import plotly.graph_objects as go
from wordcloud import WordCloud

import nltk

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
sns.set_theme(style="dark")
mpl.rcParams['axes.unicode_minus'] = False
pd.set_option('display.max_columns',None)
plt.style.use('seaborn-dark-palette')

"""#Data Exploration """

df = pd.read_csv('/content/Iris.csv')
df['No_Species'] = 0
df.loc[df['Species'] == 'Iris-setosa', 'No_Species'] = 0
df.loc[df['Species'] == 'Iris-versicolor', 'No_Species'] = 1
df.loc[df['Species'] == 'Iris-virginica', 'No_Species'] = 2
df.head(60)

df.describe()

df["Species"].value_counts()

##Checar valores no nulos
df.notnull().sum()

##Checar valores nulos
df.isnull().sum()

##Checar blancos
df.isna().sum()

plt.figure(figsize = (10,8))
sns.heatmap(pd.DataFrame(df).corr(), annot=True)
plt.title('Correlation of Variables')
plt.show()

x = df[['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm']].values
classes = {'Iris-setosa': 0, 'Iris-versicolor': 1, 'Iris-virginica': 2}
y = df['No_Species'].values  # variable dependiente
df.info()

from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import RandomForestClassifier

from sklearn import metrics
from sklearn.metrics import classification_report,confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.metrics import accuracy_score

name ='Regresion Logistica'
logistic = LogisticRegression()

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(x, y, random_state=42)

names = []
accu_scores = []
accuracies = []

logistic.fit(X_train, y_train)
y_pred = logistic.predict(X_test)

accu_scores.append(accuracy_score(y_test, y_pred))
names.append(name)
tr_split = pd.DataFrame({'Name': names, 'Score': accu_scores})
print(tr_split)

logistic.fit(X_train, y_train)
y_pred = logistic.predict(X_test)
print("Matriz de Confusión para ", name, confusion_matrix(y_test,y_pred))
print("Reporte de Clasificación para ", name, classification_report(y_test,y_pred))

from sklearn.model_selection import KFold
from sklearn.metrics import make_scorer
from sklearn.model_selection import cross_validate

kfold = KFold(n_splits=30, shuffle=True, random_state=42)
scorer = make_scorer(accuracy_score)

veracity = cross_validate(logistic, x, y, cv=kfold, scoring=scorer)

acur = veracity['test_score']
var = veracity['test_score'].var()
m = veracity['test_score'].mean()
sd = veracity['test_score'].std()


print("Accuracy of the Logistic Regretion", "model with k-fold cross validation")
print("K-fold accuracies: ", acur)
print("Mean :", m)
print("Variance: ",var )
print("Standard deviation: ", sd)
print("Bias: ", 1 - m)

from sklearn.model_selection import learning_curve
from sklearn.model_selection import ShuffleSplit

train_sizes, train_scores, test_scores = learning_curve(logistic, X_test, y_test, cv=10, scoring='neg_mean_squared_error', n_jobs=-1, train_sizes=np.linspace(0.01, 1.0, 50), verbose=0)

train_scores_mean = -train_scores.mean(axis = 1)
train_scores_std = train_scores.std(axis = 1)
test_scores_mean = -test_scores.mean(axis = 1)
test_scores_std = test_scores.std(axis = 1)


plt.figure(figsize=(25, 8))
plt.style.use('seaborn-whitegrid')
plt.title('Curva de Aprendizaje')
plt.xlabel('Tamaño del Conjunto de Entrenamiento')
plt.ylabel('Error Cuadrático Medio')
plt.grid()

plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color="r")
plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color="#2DD6A0")

plt.plot(train_sizes, train_scores_mean, 'o-', color="r", label="Entrenamiento")
plt.plot(train_sizes, test_scores_mean, 'o-', color="#2DD6A0", label="Validación Cruzada")

plt.ylim(-0.1, 1.1)
plt.legend(loc="best")
plt.show()

from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import StratifiedKFold

# Create the parameter grid based on the results of random search with iris dataset

param_grid = {
    'penalty': ['l1', 'l2', 'elasticnet', 'none'],
    'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],
    'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],
    'max_iter': [100, 1000, 2500, 5000]
}

# Instantiate the grid search model
grid_search = GridSearchCV(estimator = logistic, param_grid = param_grid,
                            cv = 3, n_jobs = -1, verbose = 2)

grid_search.fit(X_train, y_train)

grid_search.best_params_

grid_search.best_score_

grid_search.best_estimator_

name ='Regresion Logistica'
logistic = LogisticRegression(C=1, penalty='l1', solver='saga')

logistic.fit(X_train, y_train)
y_pred = logistic.predict(X_test)

print("Matriz de Confusión para ", name, confusion_matrix(y_test,y_pred))
print("Reporte de Clasificación para ", name, classification_report(y_test,y_pred))

kfold = KFold(n_splits=30, shuffle=True, random_state=42)
scorer = make_scorer(accuracy_score)

veracity = cross_validate(logistic, x, y, cv=kfold, scoring=scorer)

acur = veracity['test_score']
var = veracity['test_score'].var()
m = veracity['test_score'].mean()
sd = veracity['test_score'].std()


print("Accuracy of the Logistic Regretion", "model with k-fold cross validation")
print("K-fold accuracies: ", acur)
print("Mean :", m)
print("Variance: ",var )
print("Standard deviation: ", sd)
print("Bias: ", 1 - m)

train_sizes, train_scores, test_scores = learning_curve(logistic, X_test, y_test, cv=10, scoring='neg_mean_squared_error', n_jobs=-1, train_sizes=np.linspace(0.01, 1.0, 50), verbose=0)

train_scores_mean = -train_scores.mean(axis = 1)
train_scores_std = train_scores.std(axis = 1)
test_scores_mean = -test_scores.mean(axis = 1)
test_scores_std = test_scores.std(axis = 1)


plt.figure(figsize=(25, 8))
plt.style.use('seaborn-whitegrid')
plt.title('Curva de Aprendizaje')
plt.xlabel('Tamaño del Conjunto de Entrenamiento')
plt.ylabel('Error Cuadrático Medio')
plt.grid()

plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color="r")
plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color="#2DD6A0")

plt.plot(train_sizes, train_scores_mean, 'o-', color="r", label="Entrenamiento")
plt.plot(train_sizes, test_scores_mean, 'o-', color="#2DD6A0", label="Validación Cruzada")

plt.ylim(-0.1, 1.1)
plt.legend(loc="best")
plt.show()

def prediction(sepal_length,sepal_width,petal_length,petal_width):
    plants = {0: 'Iris-setosa', 1: 'Iris-versicolor', 2: 'Iris-virginica'}
    pred = logistic.predict([[sepal_length, sepal_width, petal_length, petal_width]])
    print("La especie de la flor es: ", pred," ",plants[pred[0]])

# Id	SepalLengthCm	SepalWidthCm	PetalLengthCm	PetalWidthCm  Species	    No_Species
# 1	  5.1	          3.5	          1.4	          0.2	          Iris-setosa	0
prediction(5.1,3.5,1.4,0.2) 
# Id	SepalLengthCm	SepalWidthCm	PetalLengthCm	PetalWidthCm	Species	    No_Species
# 51  7.0	          3.2	          4.7	          1.4	          Iris-versicolor	1
prediction(7.0,3.2,4.7,1.4) 
# Id	SepalLengthCm	SepalWidthCm	PetalLengthCm	PetalWidthCm	Species	    No_Species
# 150	5.9	          3.0	          5.1	          1.8	          Iris-virginica	2
prediction(5.9,3.0,5.1,1.8) 
# Id	SepalLengthCm	SepalWidthCm	PetalLengthCm	PetalWidthCm	Species	    No_Species
# 23	4.6	          3.6	          1.0	          0.2	          Iris-setosa	0	
prediction(4.6,3.6,1.0,0.2)
# Id	SepalLengthCm	SepalWidthCm	PetalLengthCm	PetalWidthCm	Species	    No_Species
# 24	5.1	          3.3	          1.7	          0.5	          Iris-setosa	0
prediction(5.1,3.3,1.7,0.5)